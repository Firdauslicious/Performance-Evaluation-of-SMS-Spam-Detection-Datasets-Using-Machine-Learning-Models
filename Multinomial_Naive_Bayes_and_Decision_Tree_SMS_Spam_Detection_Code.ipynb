{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NeserEFxxYvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified to remove English stopwords"
      ],
      "metadata": {
        "id": "NPG3t9VhxjBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDIfbLqvw6t5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "file_path = '{Insert file path here}'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Check for unintended data and print the exact row\n",
        "unintended_indices = df[(df['v1'] != 'ham') & (df['v1'] != 'spam')].index\n",
        "if len(unintended_indices) > 0:\n",
        "    print(\"UNINTENDED DATA found at rows:\")\n",
        "    print(df.loc[unintended_indices])\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import nltk\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = df.replace(['ham', 'spam'], [0, 1])\n",
        "\n",
        "df['Count'] = 0\n",
        "\n",
        "for i in np.arange(0, len(df['v2'])):\n",
        "    df.loc[i, 'Count'] = len(str(df.loc[i, 'v2']))\n",
        "\n",
        "corpus = []\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    msg = df['v2'][i]\n",
        "    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('£|\\$', 'moneysymb', str(df['v2'][i]))\n",
        "    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', str(df['v2'][i]))\n",
        "    msg = re.sub('\\d+(\\.\\d+)?', 'numbr', str(df['v2'][i]))\n",
        "\n",
        "    msg = re.sub('[^\\w\\d\\s]', ' ', str(df['v2'][i]))\n",
        "\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "\n",
        "    msg = [ps.stem(word) for word in msg if not word in set(stopwords.words('english'))]\n",
        "\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = df['v1']\n",
        "y = y.astype(str)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "x = pd.DataFrame(x)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "common_indices = x.index.intersection(y.index)\n",
        "x = x.loc[common_indices, :]\n",
        "y = y.loc[common_indices]\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=0)\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3],\n",
        "    'criterion': ['gini'],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "start_time_dt = time.time()\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=50)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_dt.fit(xtrain, ytrain)\n",
        "\n",
        "best_dt_params = grid_search_dt.best_params_\n",
        "print(f\"Best Decision Tree Parameters: {best_dt_params}\")\n",
        "\n",
        "dt.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_dt = dt.predict(xtest)\n",
        "\n",
        "end_time_dt = time.time()\n",
        "\n",
        "execution_time_dt = end_time_dt - start_time_dt\n",
        "execution_time_dt_rounded = round(execution_time_dt)\n",
        "\n",
        "cm_dt = confusion_matrix(ytest, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "print(\"Execution time for Decision Tree:\", execution_time_dt, \"seconds\")\n",
        "\n",
        "print(\"Decision Tree Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, dt.predict(xtest)))\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(classification_report(ytest, dt.predict(xtest)))\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [1, 2, 5]\n",
        "}\n",
        "\n",
        "start_time_nb = time.time()\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "\n",
        "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_nb.fit(xtrain, ytrain)\n",
        "\n",
        "best_nb_params = grid_search_nb.best_params_\n",
        "print(f\"Best Naive Bayes Parameters: {best_nb_params}\")\n",
        "\n",
        "nb.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_nb = nb.predict(xtest)\n",
        "\n",
        "end_time_nb = time.time()\n",
        "\n",
        "execution_time_nb = end_time_nb - start_time_nb\n",
        "\n",
        "cm_nb = confusion_matrix(ytest, y_pred_nb)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes Confusion Matrix:\")\n",
        "print(cm_nb)\n",
        "\n",
        "print(\"Execution time for Multinomial Naive Bayes:\", execution_time_nb, \"seconds\")\n",
        "execution_time_nb_rounded = round(execution_time_nb)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, nb.predict(xtest)))\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(ytest, nb.predict(xtest)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified to remove Turkish stopwords (Dataset 2)"
      ],
      "metadata": {
        "id": "XuU9ZZI6xoCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from snowballstemmer import TurkishStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "file_path = '{Insert file path here}'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "unintended_indices = df[(df['v1'] != 'ham') & (df['v1'] != 'spam')].index\n",
        "if len(unintended_indices) > 0:\n",
        "    print(\"UNINTENDED DATA found at rows:\")\n",
        "    print(df.loc[unintended_indices])\n",
        "\n",
        "df = df.replace(['ham', 'spam'], [0, 1])\n",
        "\n",
        "df['Count'] = 0\n",
        "\n",
        "for i in np.arange(0, len(df['v2'])):\n",
        "    df.loc[i, 'Count'] = len(str(df.loc[i, 'v2']))\n",
        "\n",
        "corpus = []\n",
        "stemmer = TurkishStemmer()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    msg = df['v2'][i]\n",
        "    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('£|\\$', 'moneysymb', str(df['v2'][i]))\n",
        "    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', str(df['v2'][i]))\n",
        "    msg = re.sub('\\d+(\\.\\d+)?', 'numbr', str(df['v2'][i]))\n",
        "\n",
        "    msg = re.sub('[^\\w\\d\\s]', ' ', str(df['v2'][i]))\n",
        "\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "\n",
        "    msg = [stemmer.stemWord(word) for word in msg if not word in set(stopwords.words('turkish'))]\n",
        "\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = df['v1']\n",
        "y = y.astype(str)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "x = pd.DataFrame(x)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "common_indices = x.index.intersection(y.index)\n",
        "x = x.loc[common_indices, :]\n",
        "y = y.loc[common_indices]\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=0)\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3],\n",
        "    'criterion': ['gini'],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "start_time_dt = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=50)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_dt.fit(xtrain, ytrain)\n",
        "\n",
        "best_dt_params = grid_search_dt.best_params_\n",
        "print(f\"Best Decision Tree Parameters: {best_dt_params}\")\n",
        "\n",
        "dt.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_dt = dt.predict(xtest)\n",
        "\n",
        "end_time_dt = time.time()\n",
        "\n",
        "execution_time_dt = end_time_dt - start_time_dt\n",
        "execution_time_dt_rounded = round(execution_time_dt)\n",
        "\n",
        "cm_dt = confusion_matrix(ytest, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "print(\"Execution time for Decision Tree:\", execution_time_dt, \"seconds\")\n",
        "\n",
        "print(\"Decision Tree Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, dt.predict(xtest)))\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(classification_report(ytest, dt.predict(xtest)))\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [1, 2, 5]\n",
        "}\n",
        "\n",
        "start_time_nb = time.time()\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_nb.fit(xtrain, ytrain)\n",
        "\n",
        "best_nb_params = grid_search_nb.best_params_\n",
        "print(f\"Best Naive Bayes Parameters: {best_nb_params}\")\n",
        "\n",
        "nb.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_nb = nb.predict(xtest)\n",
        "\n",
        "end_time_nb = time.time()\n",
        "\n",
        "execution_time_nb = end_time_nb - start_time_nb\n",
        "\n",
        "cm_nb = confusion_matrix(ytest, y_pred_nb)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes Confusion Matrix:\")\n",
        "print(cm_nb)\n",
        "\n",
        "print(\"Execution time for Multinomial Naive Bayes:\", execution_time_nb, \"seconds\")\n",
        "execution_time_nb_rounded = round(execution_time_nb)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, nb.predict(xtest)))\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(ytest, nb.predict(xtest)))"
      ],
      "metadata": {
        "id": "sR8Hvop2xqUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified to remove Bengali stopwords (Dataset 4)"
      ],
      "metadata": {
        "id": "a1jCqiEXxy4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bnlp_toolkit"
      ],
      "metadata": {
        "id": "Fi64PNa6x2An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import bnlp\n",
        "from bnlp import NLTKTokenizer\n",
        "from bnlp import BengaliCorpus\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "file_path = '{Insert file path here}'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "unintended_indices = df[(df['v1'] != 'ham') & (df['v1'] != 'spam')].index\n",
        "if len(unintended_indices) > 0:\n",
        "    print(\"UNINTENDED DATA found at rows:\")\n",
        "    print(df.loc[unintended_indices])\n",
        "\n",
        "df = df.replace(['ham', 'spam'], [0, 1])\n",
        "\n",
        "df['Count'] = 0\n",
        "\n",
        "for i in np.arange(0, len(df['v2'])):\n",
        "    df.loc[i, 'Count'] = len(str(df.loc[i, 'v2']))\n",
        "\n",
        "bengali_corpus = BengaliCorpus()\n",
        "\n",
        "corpus = []\n",
        "bnltk = NLTKTokenizer()\n",
        "stopwords_bn = bengali_corpus.stopwords\n",
        "\n",
        "for i in range(len(df)):\n",
        "    msg = df['v2'][i]\n",
        "    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('£|\\$', 'moneysymb', str(df['v2'][i]))\n",
        "    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', str(df['v2'][i]))\n",
        "    msg = re.sub('\\d+(\\.\\d+)?', 'numbr', str(df['v2'][i]))\n",
        "\n",
        "    msg = re.sub('[^\\w\\d\\s]', ' ', str(df['v2'][i]))\n",
        "\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "\n",
        "    msg = [word for word in msg if word not in stopwords_bn]\n",
        "\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = df['v1']\n",
        "y = y.astype(str)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "x = pd.DataFrame(x)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "common_indices = x.index.intersection(y.index)\n",
        "x = x.loc[common_indices, :]\n",
        "y = y.loc[common_indices]\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=0)\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3],\n",
        "    'criterion': ['gini'],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "start_time_dt = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=50)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_dt.fit(xtrain, ytrain)\n",
        "\n",
        "best_dt_params = grid_search_dt.best_params_\n",
        "print(f\"Best Decision Tree Parameters: {best_dt_params}\")\n",
        "\n",
        "dt.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_dt = dt.predict(xtest)\n",
        "\n",
        "end_time_dt = time.time()\n",
        "\n",
        "execution_time_dt = end_time_dt - start_time_dt\n",
        "execution_time_dt_rounded = round(execution_time_dt)\n",
        "\n",
        "cm_dt = confusion_matrix(ytest, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "print(\"Execution time for Decision Tree:\", execution_time_dt, \"seconds\")\n",
        "\n",
        "print(\"Decision Tree Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, dt.predict(xtest)))\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(classification_report(ytest, dt.predict(xtest)))\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [1, 2, 5]\n",
        "}\n",
        "\n",
        "start_time_nb = time.time()\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_nb.fit(xtrain, ytrain)\n",
        "\n",
        "\n",
        "best_nb_params = grid_search_nb.best_params_\n",
        "print(f\"Best Naive Bayes Parameters: {best_nb_params}\")\n",
        "\n",
        "nb.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_nb = nb.predict(xtest)\n",
        "\n",
        "end_time_nb = time.time()\n",
        "\n",
        "execution_time_nb = end_time_nb - start_time_nb\n",
        "\n",
        "cm_nb = confusion_matrix(ytest, y_pred_nb)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes Confusion Matrix:\")\n",
        "print(cm_nb)\n",
        "\n",
        "print(\"Execution time for Multinomial Naive Bayes:\", execution_time_nb, \"seconds\")\n",
        "execution_time_nb_rounded = round(execution_time_nb)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, nb.predict(xtest)))\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(ytest, nb.predict(xtest)))"
      ],
      "metadata": {
        "id": "RkQGsEACx3md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified to remove Indonesian stopwords (Dataset 8)"
      ],
      "metadata": {
        "id": "BE9ZOm06y3r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import time\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "file_path = '{Insert file path here}'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "unintended_indices = df[(df['v1'] != 'ham') & (df['v1'] != 'spam')].index\n",
        "if len(unintended_indices) > 0:\n",
        "    print(\"UNINTENDED DATA found at rows:\")\n",
        "    print(df.loc[unintended_indices])\n",
        "\n",
        "df = df.replace(['ham', 'spam'], [0, 1])\n",
        "\n",
        "df['Count'] = 0\n",
        "\n",
        "for i in np.arange(0, len(df['v2'])):\n",
        "    df.loc[i, 'Count'] = len(str(df.loc[i, 'v2']))\n",
        "\n",
        "corpus = []\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "for i in range(len(df)):\n",
        "    msg = df['v2'][i]\n",
        "    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('£|\\$', 'moneysymb', str(df['v2'][i]))\n",
        "    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', str(df['v2'][i]))\n",
        "    msg = re.sub('\\d+(\\.\\d+)?', 'numbr', str(df['v2'][i]))\n",
        "\n",
        "    msg = re.sub('[^\\w\\d\\s]', ' ', str(df['v2'][i]))\n",
        "\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "\n",
        "    msg = [word for word in msg if word not in stop_words]\n",
        "\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = df['v1']\n",
        "y = y.astype(str)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "x = pd.DataFrame(x)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "common_indices = x.index.intersection(y.index)\n",
        "x = x.loc[common_indices, :]\n",
        "y = y.loc[common_indices]\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=0)\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3],\n",
        "    'criterion': ['gini'],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "start_time_dt = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=50)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_dt.fit(xtrain, ytrain)\n",
        "\n",
        "best_dt_params = grid_search_dt.best_params_\n",
        "print(f\"Best Decision Tree Parameters: {best_dt_params}\")\n",
        "\n",
        "dt.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_dt = dt.predict(xtest)\n",
        "\n",
        "end_time_dt = time.time()\n",
        "\n",
        "execution_time_dt = end_time_dt - start_time_dt\n",
        "execution_time_dt_rounded = round(execution_time_dt)\n",
        "\n",
        "cm_dt = confusion_matrix(ytest, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "print(\"Execution time for Decision Tree:\", execution_time_dt, \"seconds\")\n",
        "\n",
        "print(\"Decision Tree Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, dt.predict(xtest)))\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(classification_report(ytest, dt.predict(xtest)))\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [1, 2, 5]\n",
        "}\n",
        "\n",
        "start_time_nb = time.time()\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_nb.fit(xtrain, ytrain)\n",
        "\n",
        "best_nb_params = grid_search_nb.best_params_\n",
        "print(f\"Best Naive Bayes Parameters: {best_nb_params}\")\n",
        "\n",
        "nb.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_nb = nb.predict(xtest)\n",
        "\n",
        "end_time_nb = time.time()\n",
        "\n",
        "execution_time_nb = end_time_nb - start_time_nb\n",
        "\n",
        "cm_nb = confusion_matrix(ytest, y_pred_nb)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes Confusion Matrix:\")\n",
        "print(cm_nb)\n",
        "\n",
        "print(\"Execution time for Multinomial Naive Bayes:\", execution_time_nb, \"seconds\")\n",
        "execution_time_nb_rounded = round(execution_time_nb)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, nb.predict(xtest)))\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(ytest, nb.predict(xtest)))"
      ],
      "metadata": {
        "id": "RLM78xjMy6YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified to remove Hindi stopwords (Dataset 6, Dataset 9, and Dataset 10)"
      ],
      "metadata": {
        "id": "ee4HgVvIzK4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from stopwordsiso import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "file_path = '{Insert file path here}'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "unintended_indices = df[(df['v1'] != 'ham') & (df['v1'] != 'spam')].index\n",
        "if len(unintended_indices) > 0:\n",
        "    print(\"UNINTENDED DATA found at rows:\")\n",
        "    print(df.loc[unintended_indices])\n",
        "\n",
        "df['v1'] = df['v1'].replace(['ham', 'spam'], [0, 1])\n",
        "\n",
        "df['Count'] = df['v2'].apply(lambda x: len(str(x)))\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    msg = df['v2'][i]\n",
        "    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('£|\\$', 'moneysymb', str(df['v2'][i]))\n",
        "    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', str(df['v2'][i]))\n",
        "    msg = re.sub('\\d+(\\.\\d+)?', 'numbr', str(df['v2'][i]))\n",
        "\n",
        "    msg = re.sub('[^\\w\\d\\s]', ' ', str(df['v2'][i]))\n",
        "\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "\n",
        "    stopwords_hindi = stopwords(\"hi\")\n",
        "    msg = [word for word in msg if word not in stopwords_hindi]\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = df['v1'].astype(str)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=0)\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3],\n",
        "    'criterion': ['gini'],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "start_time_dt = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=50)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_dt.fit(xtrain, ytrain)\n",
        "\n",
        "best_dt_params = grid_search_dt.best_params_\n",
        "print(f\"Best Decision Tree Parameters: {best_dt_params}\")\n",
        "\n",
        "dt.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_dt = dt.predict(xtest)\n",
        "\n",
        "end_time_dt = time.time()\n",
        "\n",
        "execution_time_dt = end_time_dt - start_time_dt\n",
        "execution_time_dt_rounded = round(execution_time_dt)\n",
        "\n",
        "cm_dt = confusion_matrix(ytest, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "print(\"Execution time for Decision Tree:\", execution_time_dt, \"seconds\")\n",
        "\n",
        "print(\"Decision Tree Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, dt.predict(xtest)))\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(classification_report(ytest, dt.predict(xtest)))\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [1, 2, 5]\n",
        "}\n",
        "\n",
        "start_time_nb = time.time()\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_nb.fit(xtrain, ytrain)\n",
        "\n",
        "best_nb_params = grid_search_nb.best_params_\n",
        "print(f\"Best Naive Bayes Parameters: {best_nb_params}\")\n",
        "\n",
        "nb.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_nb = nb.predict(xtest)\n",
        "\n",
        "end_time_nb = time.time()\n",
        "\n",
        "execution_time_nb = end_time_nb - start_time_nb\n",
        "\n",
        "cm_nb = confusion_matrix(ytest, y_pred_nb)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes Confusion Matrix:\")\n",
        "print(cm_nb)\n",
        "\n",
        "print(\"Execution time for Multinomial Naive Bayes:\", execution_time_nb, \"seconds\")\n",
        "execution_time_nb_rounded = round(execution_time_nb)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, nb.predict(xtest)))\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(ytest, nb.predict(xtest)))"
      ],
      "metadata": {
        "id": "5MG-8OyYzTQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified to remove Persian stopwords (Dataset 7)"
      ],
      "metadata": {
        "id": "2xKbDOxIznB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "id": "kTeH1tO0zpyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import hazm\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from hazm import word_tokenize, stopwords_list\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load dataset\n",
        "file_path = '{Insert file path here}'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "\n",
        "unintended_indices = df[(df['v1'] != 'ham') & (df['v1'] != 'spam')].index\n",
        "if len(unintended_indices) > 0:\n",
        "    print(\"UNINTENDED DATA found at rows:\")\n",
        "    print(df.loc[unintended_indices])\n",
        "\n",
        "\n",
        "df['v1'] = df['v1'].replace(['ham', 'spam'], [0, 1])\n",
        "\n",
        "\n",
        "df['Count'] = df['v2'].apply(lambda x: len(str(x)))\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    msg = df['v2'][i]\n",
        "    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', str(df['v2'][i]))\n",
        "    msg = re.sub('£|\\$', 'moneysymb', str(df['v2'][i]))\n",
        "    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', str(df['v2'][i]))\n",
        "    msg = re.sub('\\d+(\\.\\d+)?', 'numbr', str(df['v2'][i]))\n",
        "\n",
        "    msg = re.sub('[^\\w\\d\\s]', ' ', str(df['v2'][i]))\n",
        "\n",
        "    msg = msg.lower()\n",
        "    msg = msg.split()\n",
        "\n",
        "    stopwords = stopwords_list()\n",
        "\n",
        "    msg = [word for word in msg if word not in stopwords]\n",
        "\n",
        "    msg = ' '.join(msg)\n",
        "    corpus.append(msg)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "y = df['v1'].astype(str)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=0)\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [2, 3],\n",
        "    'criterion': ['gini'],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "start_time_dt = time.time()\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=50)\n",
        "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_dt.fit(xtrain, ytrain)\n",
        "\n",
        "best_dt_params = grid_search_dt.best_params_\n",
        "print(f\"Best Decision Tree Parameters: {best_dt_params}\")\n",
        "\n",
        "dt.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_dt = dt.predict(xtest)\n",
        "\n",
        "end_time_dt = time.time()\n",
        "\n",
        "execution_time_dt = end_time_dt - start_time_dt\n",
        "execution_time_dt_rounded = round(execution_time_dt)\n",
        "\n",
        "cm_dt = confusion_matrix(ytest, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Confusion Matrix:\")\n",
        "print(cm_dt)\n",
        "\n",
        "print(\"Execution time for Decision Tree:\", execution_time_dt, \"seconds\")\n",
        "\n",
        "print(\"Decision Tree Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, dt.predict(xtest)))\n",
        "print(\"Decision Tree Classification Report:\")\n",
        "print(classification_report(ytest, dt.predict(xtest)))\n",
        "\n",
        "param_grid_nb = {\n",
        "    'alpha': [1, 2, 5]\n",
        "}\n",
        "\n",
        "start_time_nb = time.time()\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search_nb.fit(xtrain, ytrain)\n",
        "\n",
        "best_nb_params = grid_search_nb.best_params_\n",
        "print(f\"Best Naive Bayes Parameters: {best_nb_params}\")\n",
        "\n",
        "nb.fit(xtrain, ytrain)\n",
        "\n",
        "y_pred_nb = nb.predict(xtest)\n",
        "\n",
        "end_time_nb = time.time()\n",
        "\n",
        "execution_time_nb = end_time_nb - start_time_nb\n",
        "\n",
        "cm_nb = confusion_matrix(ytest, y_pred_nb)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes Confusion Matrix:\")\n",
        "print(cm_nb)\n",
        "\n",
        "print(\"Execution time for Multinomial Naive Bayes:\", execution_time_nb, \"seconds\")\n",
        "execution_time_nb_rounded = round(execution_time_nb)\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, nb.predict(xtest)))\n",
        "print(\"Multinomial Naive Bayes Classification Report:\")\n",
        "print(classification_report(ytest, nb.predict(xtest)))"
      ],
      "metadata": {
        "id": "u6z7wFHBzru9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}